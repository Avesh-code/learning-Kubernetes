# Kubernetes Documentation
## 1. HPA [Horizotal Pods Autoscaling] 
* In Kubernetes, HPA stands for **Horizontal Pod Autoscaler**. It automatically scales the number of pods in a deployment, replica set, or stateful set based on observed resource utilization or custom metrics.
### How HPA Works
- HPA monitors metrics like CPU or memory usage across pods.
- When resource usage exceeds a defined threshold, HPA increases the number of pods.
- When usage falls below the threshold, HPA decreases the number of pods to save resources.
- It relies on the Metrics Server or other metrics providers to gather resource usage data.
- HPA runs as a control loop that checks metrics and adjusts pod counts periodically (default every 15 seconds).
### Benefits of HPA
- Ensures applications handle load spikes smoothly.
- Saves infrastructure cost by scaling down during low usage.
- Enhances application reliability and responsiveness.
### metrics Installation
- If you are using a Kind cluster install Metrics Server
```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```
- Edit the Metrics Server Deployment
```bash
kubectl -n kube-system edit deployment metrics-server
```
- Add the security bypass to deployment under `container.args`
```bash
- --kubelet-insecure-tls
- --kubelet-preferred-address-types=InternalIP,Hostname,ExternalIP
```
- Restart the deployment
```bash
kubectl -n kube-system rollout restart deployment metrics-server
```
- Verify if the metrics server is running
```bash
kubectl get pods -n kube-system
kubectl top nodes
```
* for practice i have created a new namespace, deployment and server and hpa yml files
* namespace.yml
```yml
kind: Namespace
apiVersion: v1
metadata:
  name: apache
```
* deployment.yml
```yml
kind: Deployment
apiVersion: apps/v1
metadata:
  name: apache-deployment
  namespace: apache
spec:
  replicas: 1
  selector:
    matchLabels:
      app: apache
  template:
    metadata: 
      name: apache
      labels:
        app: apache
    spec:
      containers:
      - name: apache
        image: httpd:latest
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi
```
* service.yml
```yml
kind: Service
apiVersion: v1
metadata:
  name: apache-service
  namespace: apache

spec:
  selector:
      app: apache
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: ClusterIP
```
* hpa.yml
```yml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: apache-hpa
  namespace: apache
spec:
  scaleTargetRef:
    kind: Deployment
    name: apache-deployment
    apiVersion: apps/v1

  minReplicas: 1
  maxReplicas: 5

  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 5
```
```bash
kubectl apply -f namespace.yml
kubectl apply -f deployment.yml
kubectl apply -f service.yml
sudo -E kubectl port-forward service/apache-service -n apache 82:80 --address=0.0.0.0
kubectl apply -f hpa.yml
kubectl get hpa -n apache
```

> NOTE: Here i got lots of issue with metric and i will try to resolve it.

### Stress Testing
- To see HPA in action, you can perform a stress test on your deployments. Here is an example of how to generate load on the Apache deployment using 'BusyBox':
```bash
kubectl run -i --tty load-generator --image=busybox /bin/sh
```
- Inside the container, use 'wget' to generate load:
```bash
while true; do wget -q -O- http://apache-service.default.svc.cluster.local; done
```
- This will generate continuous load on the Apache service, causing the HPA to scale up the number of pods.

- Now to check if HPA worked or not, open a same new terminal and run the following command
```bash
kubectl get hpa -w
```

> Note: Wait for few minutes to get the status reflected.

## 2. VPA:
* In Kubernetes, VPA stands for **Vertical Pod Autoscaler**. It automatically adjusts the CPU and memory resource requests and limits for containers within pods based on observed usage. Unlike Horizontal Pod Autoscaler (HPA), which scales the number of pod replicas, VPA scales resources allocated to existing pods, effectively scaling vertically.

### How VPA Works

- Continuously monitors resource usage of pods.
- Recommends or automatically updates CPU and memory requests and limits.
- Main components:
  - **Recommender**: Suggests appropriate resource allocations based on usage.
  - **Updater**: Evicts pods to apply updated requests.
  - **Admission Controller**: Adjusts resource requests when pods are created or recreated.

### VPA Modes

- **Auto**: Automatically adjusts resources by evicting and recreating pods as needed.
- **Recreate**: Updates resources by pod eviction and restart.
- **Initial**: Sets requests only at pod creation, no updates afterward.
- **Off**: Only provides recommendations, without applying changes.

### Benefits of VPA

- Ensures pods have the right CPU and memory.
- Prevents over-provisioning and waste.
- Improves application performance by adapting to workloads.
- Complements HPA by adjusting resource allocation inside pods, rather than pod count.

### Installation of VPA
- For VPA
```bash
git clone https://github.com/kubernetes/autoscaler.git
cd autoscaler/vertical-pod-autoscaler
./hack/vpa-up.sh
```
- Verify the pods on VPA
```bash
kubectl get pods -n kube-system
```
* also applied the vpa in apache
* but the chnage is there is vpa.yml
* vpa.yml
```yml
kind: VerticalPodAutoscaler
apiVersion: autoscaling.k8s.io/v1
metadata:
  name: apache-vpa
  namespace: apache

spec:
  targetRef:
    name: apache-deployment
    apiVersion: apps/v1
    kind: Deployment
  updatePolicy:
    updateMode: "Auto"
```
```bash
kubectl get vpa -n apache
kubectl delete -f vpa.yml
kubectl describe vpa <vpaname>
```
## 3. Node Affinity
* Node affinity in Kubernetes is a set of rules that allow control over scheduling pods onto specific nodes based on node labels. It provides more expressive and flexible scheduling than the simpler nodeSelector, giving administrators the ability to specify conditions, including logical operators, to match pods with nodes based on labels.
* for using this we have to config deployment.yml as per below example
```yml
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
```
